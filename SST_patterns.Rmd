---
title: "SST_patterns"
author: "Amieroh Abrahams"
date: "19 August 2019"
output: html_document
---

# Are the same upwelling signals present at different distances from the coastline and throughout the different datasets

```{r}
knitr::opts_chunk$set(
  comment = "R>",
  warning = FALSE,
  message = FALSE 
)

library(tidyverse)
library(lubridate)
library(ggpubr)
library(zoo)
library(FNN)
library(scales)
library(gridExtra)
library(circular)
library(fossil)
library(mapproj)
# library(ncdf4) # This was used to process NetCDF files in an earlier version
library(stringr)
library(doMC); doMC::registerDoMC(cores = 4)
library(fasttime)
library(xtable)
library(ncdf4) # library for processing netCDFs
library(ncdf4)
library(data.table)
library(plyr)
```

# Loading in all the data created using the code bellow

```{r}
load("Data/site_list_sub.Rdata")
load("Data/SACTN_US.RData")
load("Data/site_pixels.RData")
load("Data/OISST.RData")
OISST <- BC_avhrr_only_v2_Document_Document 
rm(BC_avhrr_only_v2_Document_Document )
load("Data/CMC.RData")
```

```{r}
# SACTN_US <- SACTN_US %>% 
#   dplyr::rename(in_situ_temp = temp)
# 
# # Visualising the data
# temp_plot <- function(df){
#   plot <- ggplot(data = df, aes(x = date, y = in_situ_temp, colour = site)) +
#     geom_line(aes(group = site)) +
#     labs(x = "", y = "Temperature (Â°C)") +
#     theme(axis.text.x = element_text(angle = 45)) +
#     theme(legend.position = "top")
# }
# 
# SACTN_plot <- temp_plot(df = SACTN_US)
# SACTN_plot


# 25th quantile
```

Different distances from the coastline

```{r}
# load("Data/site_list_sub.Rdata")
# # xtable(site_list_sub, auto = TRUE)
# west <- site_list_sub
# west$coast <- "west" # Chnages wc to west
# 
# load("Data/africa_coast.RData")
# 
# ## Downloading the bathy data from NOAA
# # Download mid-res bathymetry data
# # sa_lat <- c(-38, -24.5); sa_lon <- c(11.5, 35.5)
# # sa_bathy <- as.xyz(getNOAA.bathy(lon1 = sa_lon[1], lon2 = sa_lon[2], lat1 = sa_lat[1], lat2 = sa_lat[2], resolution = 4))
# # colnames(sa_bathy) <- c("lon", "lat", "depth")
# # sa_bathy <- sa_bathy[sa_bathy$depth <= 0,]
# # save(sa_bathy, file = "Data_P1/bathy/sa_bathy.RData")
# 
# # Loading in the newly downloaded bathymetry data
# load("Data/bathy/sa_bathy.RData")
# 
# # This function takes one site (e.g. one set of lon/lats) and calculates a shore normal transect
# shore.normal.transect <- function(site, width = 2){
#   # Find the site on the coastline and it's nearest neighbour points
#   coords <- data.frame(lon = site$lon, lat = site$lat)
#   coords2 <- knnx.index(africa_coast[,1:2], as.matrix(coords), k = 1)
#   coords3 <- data.frame(site = site$site, africa_coast[c(coords2-width, coords2+width),]) 
#   coords3 <- coords3[2:1,1:3]
#   # Define the shore normal transect bearing
#   heading <- earth.bear(coords3[1,2], coords3[1,3], coords3[2,2], coords3[2,3]) + 90
#   if(heading >= 360){
#     heading <- heading-360
#   } else {
#     heading <- heading
#   }
#   heading2 <- data.frame(site = site$site, lon = site$lon, lat = site$lat, heading)
#   return(heading2)
# }
# 
# # Creating the transects
# site_transects <- data.frame()
# for(i in 1:length(west$site)){
#  site <- west[i,]
#  site_transect <- shore.normal.transect(site, 2)
#  site_transects <- rbind(site_transects, site_transect)
# }
# 
# # Manually correcting Sea Point and Kommetjie
# site_transects$heading[4:5] <- 290 
# # save(site_transects, file = "Data/site_transects.RData")
# load("Data/site_transects.RData")
# 
# # This function takes one site (e.g. one set of lon/lats) and calculates a shore norm./subal transect
# # It then extracts a lat/ lon point every X kilometres until reaching a specified isobath
# 
# transect.pixel <- function(site, distances){
#   # Extract coordinates
#   coords <- data.frame(lon = site$lon, lat = site$lat)
#   # Find lon/ lats every X metres 
#   pixels <- data.frame()
#   # deep <- 999
#   # distance_multiplier <- 1
#   # while(deep > isobath){
#   for(i in 1:length(distances)){
#     coords2 <- as.data.frame(destPoint(p = coords, b = site$heading, d = distances[i]))
#     sitesIdx <- knnx.index(sa_bathy[,1:2], as.matrix(coords2), k = 1)
#     bathy2 <- sa_bathy[sitesIdx,]
#     bathy2 <- bathy2[complete.cases(bathy2$depth),]
#     bathy3 <- data.frame(site = site$site, lon = bathy2$lon, lat = bathy2$lat, 
#                          heading = site$heading, 
#                          distance = distances[i])
#     pixels <- rbind(pixels, bathy3)
#     coords <- coords2
#   }
#   if(nrow(pixels) < 1){
#     pixels <- data.frame(site, depth = NA)
#   }else{
#     pixels <- pixels
#   }+

#   return(pixels)
# }
# 
# # Pixel points
# site_pixels <- data.frame()
# for(i in 1:length(west$site)){
#   site <- site_transects[i,]
#   site_pixel <- transect.pixel(site, c(10000, 20000, 30000, 40000, 50000)) # RWS: fixed error
#   site_pixels <- rbind(site_pixels, site_pixel)
# }
# 
# # Bounding box
#   # Only one is made in order to know how large the the geom_point() squares should be made to match
# bbox <- data.frame(xmin = destPoint(p = site_pixels[1,2:3], b = 270, d = 12500)[1],
#                    xmax = destPoint(p = site_pixels[1,2:3], b = 90, d = 12500)[1],
#                    ymin = destPoint(p = site_pixels[1,2:3], b = 180, d = 12500)[2],
#                    ymax = destPoint(p = site_pixels[1,2:3], b = 0, d = 12500)[2])
# 
# # Determining the temperature at the various distances from the coast
# 
# # save(site_pixels, file = "Data/site_pixels.RData")
# load("Data/site_pixels.RData")
```

# Extrcting the MUR data

```{r}
# library(stringr)
# library(tidyverse)
# library(reshape2)
# library(ncdf4) # library for processing netCDFs
# library(plyr)
# library(lubridate)
# library(data.table)
# library(doMC); doMC::registerDoMC(cores = 7)
# 
# ncDir <- "/home/amieroh/Documents/Data/Datasets/MUR/daily"
# csvDir <- "/home/amieroh/Documents/Data/Datasets/MUR/Extracted_MUR"
# 
# #          1         2         3         4
# # 12345678901234567890123456789012345678901
# # 20020601-JPL-L4UHfnd-GLOB-v01-fv04-MUR.nc
# 
# read_nc <- function(ncDir = ncDir, csvDir = csvDir) 
#   ncList <- list.files(path = paste0(ncDir), pattern = "*.nc", full.names = TRUE, include.dirs = TRUE)
#   ncFirst <- head(list.files(path = paste0(ncDir, "/"), pattern = "*.nc", full.names = FALSE), 1)
#   ncLast <- tail(list.files(path = paste0(ncDir, "/"), pattern = "*.nc", full.names = FALSE), 1)
#   strtDate <- str_sub(ncFirst, start = 1, end = 8)
#   endDate <- str_sub(ncLast, start = 1, end = 8)
# 
# # ncFile <- '/home/amieroh/Documents/Data/Datasets/MUR/daily/20020606-JPL-L4UHfnd-GLOB-v01-fv04-MUR.nc'
# 
#   ncFun <- function(ncFile = ncFile, csvDir = csvDir) {
#     nc <- nc_open(ncFile)
#     pathLen <- nchar(paste0(ncDir, "/")) + 1
#     fNameStem <-
#       substr(basename(ncFile), 10, 38)
#     fDate <- substr(basename(ncFile), 1, 8)
#     sst <- ncvar_get(nc, varid = "analysed_sst") %>%
#       round(4)
#     dimnames(sst) <- list(lon = nc$dim$lon$vals,
#                           lat = nc$dim$lat$vals)
#     nc_close(nc)
#     sst <- as_tibble(melt(sst, value.name = "temp"))
#     sst$t <- ymd(fDate)
#     na.omit(sst)
#     fwrite(sst,
#            file = paste0(csvDir, "/", fNameStem, "-", strtDate, "-", endDate, ".csv"),
#            append = TRUE, col.names = FALSE)
#     rm(sst)
#   }
# 
# llply(ncList, ncFun, csvDir = csvDir, .parallel = TRUE)
# 
# 
# MUR <- read_csv("/home/amieroh/Documents/Data/Datasets/MUR/Extracted_MUR/BC-JPL-L4UHfnd-GLOB-v01-fv04-MUR-20020601-20140727.csv")
# names(MUR)<-c("lon","lat", "temp", "date")
# MUR <- MUR %>% 
#   mutate(temp = temp - 273.15)
# 
# JPL_L4UHfnd_GLOB_v01_fv04_MUR_20020601_20140727 <- read_csv("~/Documents/JPL-L4UHfnd-GLOB-v01-fv04-MUR-20020601-20140727.csv")
# 
# ######################################### SUBSET VIA REGION #################
# 
# # bbox <- data.frame(BC = c(-35, -25, 15, 20), # Benguela Current
# library(stringr)
# library(tidyverse)
# library(reshape2)
# library(ncdf4) # library for processing netCDFs
# library(plyr)
# library(lubridate)
# library(data.table)
# library(doMC); doMC::registerDoMC(cores = 7)
# ncDir <- "/home/amieroh/Documents/Data/Datasets/MUR/daily"
# csvDir <- "/home/amieroh/Documents/Data/Datasets/MUR/Extracted_MUR"
# #          1         2         3         4
# # 12345678901234567890123456789012345678901
# # 20020601-JPL-L4UHfnd-GLOB-v01-fv04-MUR.nc
# 
# # ncFile <- '/home/amieroh/Documents/Data/Datasets/MUR/daily/20020606-JPL-L4UHfnd-GLOB-v01-fv04-MUR.nc'
#   ncFun <- function(ncFile = ncFile, region = region, csvDir = csvDir) {
#     coords <- bbox[, region]
#     nc <- nc_open(ncFile)
#     pathLen <- nchar(paste0(ncDir, "/")) + 1
#     fNameStem <-
#       substr(basename(ncFile), 10, 38)
#     fDate <- substr(basename(ncFile), 1, 8)
#     LatIdx <- which(nc$dim$lat$vals > coords[1] & nc$dim$lat$vals < coords[2])
#     LonIdx <- which(nc$dim$lon$vals > coords[3] & nc$dim$lon$vals < coords[4])
#     sst <- ncvar_get(nc, varid = "analysed_sst") %>%
#       round(4)
#     dimnames(sst) <- list(lon = nc$dim$lon$vals,
#                           lat = nc$dim$lat$vals)
#     nc_close(nc)
#     sst <- as_tibble(melt(sst, value.name = "temp"))
#     sst$t <- ymd(fDate)
#     na.omit(sst)
#     fwrite(sst,
#            file = paste0(csvDir, "/", region, "-", fNameStem, "-", strtDate, "-", endDate, ".csv"),
#            append = TRUE, col.names = FALSE)
#     rm(sst)
#   }
#   
#   
#   ncList <- list.files(path = paste0(ncDir), pattern = "*.nc", full.names = TRUE, include.dirs = TRUE)
#   ncFirst <- head(list.files(path = paste0(ncDir, "/"), pattern = "*.nc", full.names = FALSE), 1)
#   ncLast <- tail(list.files(path = paste0(ncDir, "/"), pattern = "*.nc", full.names = FALSE), 1)
#   strtDate <- str_sub(ncFirst, start = 1, end = 8)
#   endDate <- str_sub(ncLast, start = 1, end = 8)  
#   
# llply(ncList, ncFun, region = "BC", csvDir = csvDir, .parallel = TRUE)
```

# Extracting the CMC data

```{r}
# ncDir <- "/home/amieroh/Documents/Data/Datasets/CMC/CMC_BC"
# csvDir <- "/home/amieroh/Documents/Data/Datasets/CMC/CMC_extracted"
# 
# #          1         2         3         4         5         6
# # 123456789012345678901234567890123456789012345678901234567890
# # 20100609-JPL_OUROCEAN-L4UHfnd-GLOB-v01-fv01_0-G1SST_subset.nc
# read_nc <- function(ncDir = ncDir, csvDir = csvDir) 
#   ncList <- list.files(path = paste0(ncDir), pattern = "*.nc", full.names = TRUE, include.dirs = TRUE)
#   ncFirst <- head(list.files(path = paste0(ncDir, "/"), pattern = "*.nc", full.names = FALSE), 1)
#   ncLast <- tail(list.files(path = paste0(ncDir, "/"), pattern = "*.nc", full.names = FALSE), 1)
#   strtDate <- str_sub(ncFirst, start = 1, end = 8)
#   endDate <- str_sub(ncLast, start = 1, end = 8)
# 
# # ncFile <- '/home/amieroh/Documents/Data/Datasets/CMC/CMC_BC/20100609-JPL_OUROCEAN-L4UHfnd-GLOB-v01-fv01_0-G1SST_subset.nc'
# 
#   ncFun <- function(ncFile = ncFile, csvDir = csvDir) {
#     nc <- nc_open(ncFile)
#     pathLen <- nchar(paste0(ncDir, "/")) + 1
#     fNameStem <-
#       substr(basename(ncFile), 10, 58)
#     fDate <- substr(basename(ncFile), 1, 8)
#     sst <- ncvar_get(nc, varid = "analysed_sst") %>%
#       round(4)
#     dimnames(sst) <- list(lon = nc$dim$lon$vals,
#                           lat = nc$dim$lat$vals)
#     nc_close(nc)
#     sst <- as_tibble(melt(sst, value.name = "temp"))
#     sst$t <- ymd(fDate)
#     na.omit(sst)
#     fwrite(sst,
#            file = paste0(csvDir, "/", fNameStem, "-", strtDate, "-", endDate, ".csv"),
#            append = TRUE, col.names = FALSE)
#     rm(sst)
#   }
# 
# llply(ncList, ncFun, csvDir = csvDir, .parallel = TRUE)
# 
# 
# CMC <- read_csv("/home/amieroh/Documents/Data/Datasets/CMC/CMC_extracted/Benguela_current/20000-CMC-L4_GHRSST-SSTfnd-CMC0.2deg-GLOB-v02.0-f-19910901-20170317.csv")
# names(CMC)<-c("lon","lat", "temp", "date")
# CMC <- CMC %>% 
#   mutate(temp = temp - 273.15)
# 
# # save(CMC, file = "Data/CMC.RData")
```

# Extracting the OISST data

```{r}
# bbox <- data.frame(BC = c(-35, -25, 15, 20), # Benguela Current
#                    CC = c(25, 35, 340, 355), # Canary Current
#                    CalC = c(35, 45, 225, 240), # California Current
#                    HC = c(-17.5, -7.5, 275, 290), # Humboldt Current
#                    row.names = c("latmin", "latmax", "lonmin", "lonmax"))
# 
# OISST.dir <- "/home/amieroh/Documents/Data/Datasets/OISSTv2/daily/netCDF/avhrr-only"
# OISST.csv.dir <- "/home/amieroh/Documents/Data/Datasets/OISST_subset"
# 
# #          1         2
# # 1234567890123456789012345
# # avhrr-only-v2.19810901.nc
# 
# # function to extract the dims and data from OISST netCDFs
# read_nc <- function(ncFile, region = region, csvDir = csvDir) {
#   coords <- bbox[, region]
#   nc <- nc_open(ncFile)
#   pathLen <- nchar(OISST.dir) + 1 # to account for the "/" that needs to be inserted
#   fNameStem <-
#     substr(ncFile, pathLen + 1, pathLen + 13)
#   fDate <- substr(ncFile, pathLen + 15, pathLen + 22)
#   LatIdx <- which(nc$dim$lat$vals > coords[1] & nc$dim$lat$vals < coords[2])
#   LonIdx <- which(nc$dim$lon$vals > coords[3] & nc$dim$lon$vals < coords[4])
#   sst <- ncvar_get(nc,
#                    varid = "sst",
#                    start = c(LonIdx[1], LatIdx[1], 1, 1),
#                    count = c(length(LonIdx), length(LatIdx), 1, 1)) %>%
#     round(4)
#   dimnames(sst) <- list(lon = nc$dim$lon$vals[LonIdx],
#                         lat = nc$dim$lat$vals[LatIdx])
#   nc_close(nc)
#   sst <-
#     as.data.table(melt(sst, value.name = "temp"), row.names = NULL) %>%
#     mutate(t = ymd(fDate)) %>%
#     na.omit()
#   fwrite(sst,
#          file = paste(csvDir, "/", region, "-", fNameStem, ".", strtDate, "-", endDate, ".csv", sep = ""),
#          append = TRUE, col.names = FALSE)
#   rm(sst)
# }
# 
# # the list of files
# ncList <- list.files(path = OISST.dir, pattern = "*.nc", full.names = TRUE, include.dirs = TRUE)
# strtDate <- str_sub(ncList[1], start = 15, end = 22)
# endDate <- str_sub(ncList[length(ncList)], start = 15, end = 22)
# 
# # apply the function
# system.time(llply(ncList, read_nc, region = "BC", csvDir = OISST.csv.dir, .parallel = TRUE))
# # system.time(llply(ncList, read_nc, region = "CC", csvDir = OISST.csv.dir, .parallel = TRUE))
# # system.time(llply(ncList, read_nc, region = "CalC", csvDir = OISST.csv.dir, .parallel = TRUE))
# # system.time(llply(ncList, read_nc, region = "HC", csvDir = OISST.csv.dir, .parallel = TRUE))
# 
# # Loading the data
# BC_avhrr_only_v2_Document_Document <- read_csv("~/Documents/OISST_subset/BC-avhrr-only-v2.Document-Document.csv" )
# names(BC_avhrr_only_v2_Document_Document)<-c("lon","lat", "temp", "date")
# 
# # Saving the data
# save(BC_avhrr_only_v2_Document_Document, file = "Data/OISST.RData")
```

# Using the upwelling metrics created in `upwell.IDX.Rmd` Identify when upwelling occurs at the particular site. 
# Is this upwelling event seen throughout the different distances from the coastline
# Using CMC, MUR, OISST and SACTN_US to determine wether the upwelling events detected are present in each of the datasets

## Function
Determining the temperatures at the various distances from the coastline

```{r}
# RWS: It appears as though the satellite data you are loading here have already been trimmed down to the one nearest pixel to each SACTN site
# This won't work for the following steps because you need to be finding pixels from the COMPLETE set of satellite data you downloaded
# Once you load the complete satellite data lon/lat values you can run the following code and it should give you the coordinates you want



# These following three objects (MUR, OISST, CMC) need to be the complete set of lon/lat values for your satellite data
# MUR <- MUR %>% 
#   select(lon, lat) %>% 
#   mutate(product = "MUR")

OISST <- OISST %>% 
  select(lon, lat) %>% 
  mutate(product = "OISST")

CMC <- CMC %>% 
  select(lon, lat) %>% 
  mutate(product = "CMC")

sat_data <- rbind(CMC, OISST) %>% 
  rbind(., CMC) %>% 
  select(product, lon, lat)

# rm(MUR, OISST, CMC); gc()

sat_pixels <- sat_data %>% 
  select(product, lon, lat) %>%
  unique()

## For testing the nest/map pipeline
# df <- sat_pixels %>% 
#   filter(product == "MUR", site == "Lamberts Bay") %>% 
#   select(-c(product, site))

match_func <- function(df){
  df <- na.omit(df) %>% 
    dplyr::rename(lon_sat = lon, lat_sat = lat)
  match_index <- knnx.index(data = as.matrix(site_pixels[,2:3]),
                    query = as.matrix(df[,1:2]), k = 1)
  res <- cbind(site_pixels[match_index,], df)
  return(res)
}

pixel_match <- sat_data %>% 
  group_by(product) %>% 
  nest() %>% 
  mutate(match = purrr:::map(data, match_func)) %>% 
  select(-data) %>%
  unnest(match)

pixel_match <- pixel_match %>%
  unite(col = combi, lon_sat, lat_sat, sep = "/", remove = F)
  

# You may then use the 'pixel_match' object to filter out the desired pixels from the full satellite products
temp_match <- function(df){
  temp <- df %>%
  unite(col = combi, lon, lat, sep = "/", remove = F) %>%
  filter(combi %in% pixel_match$combi)
}

OISST_temp <- temp_match(df = OISST)
CMC_temp <- temp_match(df = CMC)


# You will then have one time series for each product for each pixel at each distance for each site that you may then
# run the upwelling event function on
# You should do this via purrr::nest+map as you can see how much less code it requires
```






















