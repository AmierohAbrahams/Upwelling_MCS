---
title: "Appendices"
author: "Amieroh Abrahams"
date: "08 October 2019"
output: html_document
---
```{r prelim_opts, echo=FALSE}
knitr::opts_chunk$set(
  comment = "R>",
  warning = FALSE,
  message = FALSE 
)


# Disable scientific notation for numeric values
# I just find it annoying
options(scipen = 999)

library(tidyverse)
library(lubridate)
library(ggpubr); theme_set(theme_pubr())
library(zoo)
library(data.table)
library(heatwaveR)
library(viridis)
library(gridExtra)
library(fossil)
library(stringr)
library(doMC); doMC::registerDoMC(cores = 4)
library(fasttime)
library(xtable)
library(plyr)
library(ggpubr)
library(zoo)
library(FNN)
library(forecast)
library(astrochron)
library(WaveletComp)
library(data.table)
library(viridis)
library(ggrepel)
library(plyr)
library(maptools)
library(sp)
library(geosphere)
library(PBSmapping)
library(scales)
library(grid)
library(gridExtra)
library(fossil)
library(mapproj)
library(ncdf4)
library(reshape2)
library(plyr) # Never load plyr when also loading the tidyverse. It causes a lot of conflicts.
library(circular)
# devtools::install_github("robwschlegel/coastR")
library(gridExtra)
library(geosphere)
library(tidyverse)
library(heatwaveR)
# install_github("marchtaylor/sinkr")

## Functions
source("Functions/earthdist.R")
source("Functions/wind.rose.R")
source("Functions/theme.R")
source("Functions/scale.bar.func.R")
```

# Loading the data and finding a 30year time series

```{r}
load("Data/SACTN_daily_v4.2.RData")
load("Data/site_list_v4.2.RData")

site_list_sub <- site_list %>%
  filter(coast == "wc") %>%
  filter(length > 10950) 
#site_list_sub <- site_list_sub[-5,]
save(site_list_sub, file = "Data/site_list_sub.Rdata")

SACTN_US <- SACTN_daily_v4.2 %>%
  left_join(site_list[,c(4,13)], by = "index") %>%
  filter(index %in% site_list_sub$index) %>%
  separate(index, into = c("site", "src"), sep = "/", remove = FALSE)

save(SACTN_US, file = "Data/SACTN_US.RData")
```

```{r}
load("Data/site_list_sub.Rdata")
load("Data/africa_coast.RData")
xtable(site_list_sub, auto = TRUE)
west <- site_list_sub[-5,]
west$coast <- "west" # Chnages wc to west

site_list_sub <- site_list_sub[-5,]


load("Data/sa_bathy.RData")
## Downloading the bathy data from NOAA
# Download mid-res bathymetry data
# sa_lat <- c(-38, -24.5); sa_lon <- c(11.5, 35.5)
# sa_bathy <- as.xyz(getNOAA.bathy(lon1 = sa_lon[1], lon2 = sa_lon[2], lat1 = sa_lat[1], lat2 = sa_lat[2], resolution = 4))
# colnames(sa_bathy) <- c("lon", "lat", "depth")
# sa_bathy <- sa_bathy[sa_bathy$depth <= 0,]
# save(sa_bathy, file = "Data_P1/bathy/sa_bathy.RData")

# Loading in the newly downloaded bathymetry data
load("Data/sa_bathy.RData")
load("Data/africa_coast.RData")

# This function takes one site (e.g. one set of lon/lats) and calculates a shore normal transect
shore.normal.transect <- function(site, width = 2){
  # Find the site on the coastline and it's nearest neighbour points
  coords <- data.frame(lon = site$lon, lat = site$lat)
  coords2 <- knnx.index(africa_coast[,1:2], as.matrix(coords), k = 1)
  coords3 <- data.frame(site = site$site, africa_coast[c(coords2-width, coords2+width),]) 
  coords3 <- coords3[2:1,1:3]
  # Define the shore normal transect bearing
  heading <- earth.bear(coords3[1,2], coords3[1,3], coords3[2,2], coords3[2,3]) + 90
  if(heading >= 360){
    heading <- heading-360
  } else {
    heading <- heading
  }
  heading2 <- data.frame(site = site$site, lon = site$lon, lat = site$lat, heading)
  return(heading2)
}

# Creating the transects
site_transects <- data.frame()
for(i in 1:length(west$site)){
 site <- west[i,]
 site_transect <- shore.normal.transect(site, 2)
 site_transects <- rbind(site_transects, site_transect)
}

# Manually correcting Sea Point and Kommetjie
site_transects$heading[4] <- 290 
# save(site_transects, file = "Data/site_transects.RData")
load("Data/site_transects.RData")

# This function takes one site (e.g. one set of lon/lats) and calculates a shore normal transect
# It then extracts a lat/ lon point every X kilometres until reaching a specified isobath

transect.pixel <- function(site, distances){
  # Extract coordinates
  coords <- data.frame(lon = site$lon, lat = site$lat)
  # Find lon/ lats every X metres 
  pixels <- data.frame()
  # deep <- 999
  # distance_multiplier <- 1
  # while(deep > isobath){
  for(i in 1:length(distances)){
    coords2 <- as.data.frame(destPoint(p = coords, b = site$heading, d = distances[i]))
    sitesIdx <- knnx.index(sa_bathy[,1:2], as.matrix(coords2), k = 1)
    bathy2 <- sa_bathy[sitesIdx,]
    bathy2 <- bathy2[complete.cases(bathy2$depth),]
    bathy3 <- data.frame(site = site$site, lon = bathy2$lon, lat = bathy2$lat, 
                         heading = site$heading, 
                         distance = distances[i])
    pixels <- rbind(pixels, bathy3)
    coords <- coords2
  }
  if(nrow(pixels) < 1){
    pixels <- data.frame(site, depth = NA)
  }else{
    pixels <- pixels
  }
  return(pixels)
}

# Pixel points
site_pixels <- data.frame()
for(i in 1:length(west$site)){
  site <- site_transects[i,]
  site_pixel <- transect.pixel(site, c(10000, 20000, 30000, 40000, 50000)) # RWS: fixed error
  site_pixels <- rbind(site_pixels, site_pixel)
}

# save(site_pixels, file = "Data/site_pixels.RData")
load("Data/site_pixels.RData")

# Bounding box
  # Only one is made in order to know how large the the geom_point() squares should be made to match
bbox <- data.frame(xmin = destPoint(p = site_pixels[1,2:3], b = 270, d = 12500)[1],
                   xmax = destPoint(p = site_pixels[1,2:3], b = 90, d = 12500)[1],
                   ymin = destPoint(p = site_pixels[1,2:3], b = 180, d = 12500)[2],
                   ymax = destPoint(p = site_pixels[1,2:3], b = 0, d = 12500)[2])

# Determining the temperature at the various distances from the coast
```


```{r , echo= FALSE, message=FALSE, warning=FALSE, fig.cap="Map representing the study region along the South African coastline. The black points represent the location of their *in situ* temperatures and approximations of the pixels used along the shore normal trasect from the satellite sea surface temperatures shown with black boxes", fig.height=6, fig.width=10, fig.pos="H"}

load("Data/south_africa_coast.RData")
load("Data/africa_coast.RData")
load("Data/sa_provinces_new.RData")
load("Data/site_list_sub.Rdata")
load("Data/site_pixels.RData")
load("MUR.RData")
names(south_africa_coast)[1] <- "lon"

# Manually divide up coastline
wc <- south_africa_coast[291:410,]

site_list_sub <- site_list_sub[-5,]


# Setting up the theme
theme_set(theme_bw())
limits <- c(12,28) # for colour bar
#breaks <- seq(6, 30, 2) # Create breaks to be used for colour bar

# Define plotting parameters
sa_lats <- c(-37, -27); sa_lons <- c(14, 25)

site_map <- ggplot(data = south_africa_coast, aes(x = lon, y = lat)) + bw_update +
  geom_raster(data = MUR, aes(x = lon, y = lat, fill = temp)) +
   geom_polygon(data = south_africa_coast, aes(x = lon, y = lat, group = group),
                fill = "grey", colour = "grey", size = 0.1, show.legend = FALSE) +
  geom_point(data = site_list_sub, aes(x = lon, y = lat), alpha = 0.8, size = 3) +
  geom_point(data = site_pixels, aes(x = lon, y = lat), colour = "white", shape = 0, alpha = 0.8, size = 2.1) +
  coord_equal(xlim = c(15, 25), ylim = c(-36,-26), expand = 0) +
  # geom_rect(data = bbox, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
  #           alpha = 0.4, colour = "red", size = 0.1, linetype = 1) +
  # scale_x_continuous(limits = sa_lons, expand = c(0, 0), breaks = seq(15, 35, 5)) +
  # scale_y_continuous(limits = sa_lats, expand = c(0, 0), breaks = seq(-35, -30, 5)) +
  scale_fill_viridis(
    option = "magma",
    name = "Temperature (°C)",
    alpha = 0.8) +
  xlab("") + ylab("") +
  # annotate("text", label = "INDIAN\nOCEAN", x = 32.00, y = -35.0,
  #            size = 4, angle = 0, colour = "black") +
    annotate("text", label = "ATLANTIC\nOCEAN", x = 17.00, y = -35.0,
             size =4, angle = 0, colour = "black") +
    annotate("text", label = "Benguela", x = 16.0, y = -31.7,
             size = 4, angle = 302, colour = "black") +
    # annotate("text", label = "Agulhas", x = 31.7, y = -31.7,
    #          size = 5, angle = 50, colour = "black") +
  guides(fill = guide_colourbar()) +
  scale_x_continuous(expand = c(0, 0),
                       labels = scales::unit_format(unit = "°E", sep = "")) +
    scale_y_continuous(expand = c(0, 0),
                       labels = scales::unit_format(unit = "°S", sep = "")) +
  theme(panel.background = element_rect(fill = "ivory", colour = NA),
        panel.border = element_rect(colour = "black", size = 0.5),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.text = element_text(size = 14),
        legend.title = element_text(size = 10),
        legend.key = element_blank(),
        legend.background = element_blank(),
        axis.ticks = element_line(size = 0.5),
        axis.text = element_text(size = 10)) +
    guides(fill = guide_colourbar(barheight = 1.00, barwidth = 10)) +
  theme(panel.background = element_rect(fill = "ivory", colour = NA),
        panel.border = element_rect(colour = "black", size = 0.5),
        panel.grid.minor = element_line(colour = "NA"),
        panel.grid.major = element_line(colour = "ivory", size = 0.2, linetype = "dotted"),
        legend.direction = "horizontal",
        legend.justification = c(1,0),
        legend.position = c(0.85, 0.70),
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 8),
        legend.key = element_blank(),
        legend.background = element_blank(),
        axis.ticks = element_line(size = 0.5))
site_map 

pdf("Figures/site_map.pdf", width = 10, height = 7, pointsize = 4) # Set PDF dimensions


africa_map <- ggplot(africa_coast, aes(x = lon, y = lat)) + 
  theme_bw() + 
  coord_equal() + 
  geom_polygon(aes(group = group), colour = "black", fill = "grey80") + 
  geom_polygon(data = sa_provinces_new, (aes(group = group))) +
  annotate("text", label = "Africa", x = 16.0, y = 15.0, size = 3) + 
  theme(panel.border = element_rect(colour = "black", size = 0.4), 
        plot.background = element_blank(), 
        axis.ticks = element_blank(), 
        axis.text = element_blank(), 
        axis.title = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  coord_map(xlim = c(-20, 53), ylim = c(-36, 38), projection = "mercator") 
africa_map

# More mapping - http://r-nold.blogspot.com/2014/06/creating-inset-map-with-ggplot2.html

vp1 <- viewport(x = 0.70, y = 0.58, w = 0.25, h = 0.25) # Africa
vp2 <- viewport(x = 1.0, y = 1.0, w = 1.00, h = 1.00, just = c("right", "top"))  # South Africa
print(site_map, vp = vp2)
print(africa_map, vp = vp1)
dev.off()
pdf("Figures/map_fixed.pdf", width = 10, height = 7, pointsize = 4) # Set PDF dimensions

# Site map
# The insetting needs to be placed at appropriate location
```

Loading the Wind data. The wind dataset was supplied by the SAWS in .text format. Wind data was provided for the sites closes to the requested site as wind data may not be available for the 6 study sites in this research.  The wind direction was collected every 2 hours and by using the circular function we calcuated the daily wind direction and mean speed. There is a numerically large gap between 360 and 2 where as in degrees its not as large. The circular mean function returns the mean direction of a vector of circular data. https://cran.r-project.org/web/packages/circular/circular.pdf (Pg 130). The circular function creates circular objects around the wind direction. 

```{r}
wind_1 <- read.delim("Data/Wind_data/wind_data.txt(SAWS)/wind1/wind1.txt", na.strings = "", 
                     col.names = c("station_number", "station_name", "date", "hour", "sub", "speed", "dir"))

wind_2 <- read.delim("Data/Wind_data/wind_data.txt(SAWS)/wind2/wind2.txt", na.strings = "",
                     col.names = c("station_number", "station_name", "date", "hour", "sub" ,"speed", "dir"))

wind_3 <- read.delim("Data/Wind_data/wind_data.txt(SAWS)/wind3/wind3.txt", na.strings = "",
                     col.names = c("station_number", "station_name", "date", "hour", "sub" ,"speed", "dir"))

# Slecting the important columns for each of the datasets
wind_fix <- function(df){
wind <- df %>% 
  select(station_name, date, hour, dir, speed) %>%  #column names
  mutate(date = as.Date(as.character(date)),
         hour = as.numeric(as.character(hour)), 
         dir = as.numeric(as.character(dir)),
         speed = as.numeric(as.character(speed)))
}
# RWS: We can see when we force the values to be numeric that there are some non-numeric values in the base data
wind_1 <- wind_fix(df = wind_1)
wind_2 <- wind_fix(df = wind_2)
wind_3 <- wind_fix(df = wind_3)

## Renaming the sites within the wind datasets to match the name of the sites at which seawater temperature was collected
## The wind data was obtained from the SAWS and the wind stations used were the closes stations to which temperature was collected

renaming_sites_1 <- function(df) {
  sites <- df %>%
      # RWS: An alternative way to replace values without having to use multiple ifelse() statements
      mutate(temp_sites = case_when(station_name == "CAPE TOWN TABLE BAY" ~ "Sea Point",
                                    station_name == "CAPE TOWN - ROYAL YACHT CLUB" ~ "Sea Point",
                                    station_name  == "PORT NOLLOTH" ~"Port Nolloth",
                                    station_name  == "CAPE TOWN SLANGKOP" ~ "Hout Bay",
                                    station_name  == "LAMBERTSBAAI NORTIER" ~ "Lamberts Bay",
                                    station_name  == "LANGEBAANWEG AWS" ~ "Saldanha Bay"))
  return(sites)
}


wind_sitesmatched_1 <-  renaming_sites_1(df = wind_1)
wind_sitesmatched_2 <-  renaming_sites_1(df = wind_2)
wind_sitesmatched_3 <-  renaming_sites_1(df = wind_3)
wind_data <- rbind(wind_sitesmatched_3,wind_sitesmatched_2,wind_sitesmatched_1)
save(wind_data, file = "Data/wind_data.RData")

# # RWS: You should never be removing values by specific call like this
#   # Rather you should be able to use some sort of conditional to screen out unwanted values
# wind_sitesmatched_1 <- wind_sitesmatched_1[-c(121352, 121353, 379892, 379893, 609324, 609325, 843506, 843507, 1014585), ]
# renaming_sites_2 <- function(df) {
#   sites <- df %>%
#     mutate(temp_sites = ifelse(station_name %in% c("ROBBENEILAND"), "Koeberg Basin",        
#                            ifelse(station_name %in% c("GEELBEK"), "Yzerfontein",
#                                 ifelse(station_name %in% c("DASSEN ISLAND"), "Dassen Island",
#                                        ifelse(station_name %in% c("ATLANTIS"), "Koeberg Basin","Error")))))
#   return(sites)
# }
# 
# wind_sitesmatched_2 <-  renaming_sites_2(df = wind_2)
# wind_sitesmatched_2 <- wind_sitesmatched_2[-c(228372, 228373, 313441, 313442, 364881, 364882, 557392), ] 
# renaming_sites_3 <- function(df) {
#   sites <- df %>%
#     mutate(temp_sites = ifelse(station_name %in% c("CAPE TOWN SLANGKOP"), "Kommetjie",  
#                            ifelse(station_name %in% c("CAPE TOWN - ROYAL YACHT CLUB"), "Sea Point",
#                            ifelse(station_name %in% c("CAPE TOWN TABLE BAY"), "Sea Point","Error"))))
#   return(sites)
# }
# 
# wind_sitesmatched_3 <-  renaming_sites_3(df = wind_3)
# wind_sitesmatched_3 <- wind_sitesmatched_3[-c(119317, 119318, 196551, 196552, 346759), ] 
### CAPE TOWN SLANGKOP may be used for Kommetjie and for Houtbay
# wind_3_HoutBay <- wind_3 %>% 
#   filter(station_name == "CAPE TOWN SLANGKOP") %>% 
#   mutate(temp_sites = ifelse(station_name %in% c("CAPE TOWN SLANGKOP"), "Hout Bay","Error"))
# wind_data <- rbind(wind_3_HoutBay,wind_sitesmatched_3,wind_sitesmatched_2,wind_sitesmatched_1)
# wind_data <- wind_data %>% 
#   na.omit()
# 
# wind_data <- wind_data %>% 
#   # mutate(date = as.Date(date)) %>%  
#   group_by(date, hour, temp_sites) 
# 
# # write.csv(wind_data, file = "Data_P1/wind_data.csv", row.names = T)
# # save(wind_data, file = "Data_P1/wind_data.RData")

# load("Data_P1/insitu_wind.RData")
# 
# test1 <- insitu_wind %>% 
#   mutate(lat = case_when(site == "Port Nolloth" ~ "-29.25241",
#                                     site == "Saldanha Bay" ~ "-33.01054",
#                                     site  == "Yzerfontein" ~ "-33.36068",
#                                     site  == "Kommetjie" ~ "-34.13667",
#                                     site  == "Lamberts Bay" ~ "-32.09185",
#                                     site  == "Sea Point" ~ "-33.91847"))
# 
# test <- test1 %>% 
#   mutate(lon = case_when(site == "Port Nolloth" ~ "16.86710",
#                                     site == "Saldanha Bay" ~ "17.95063",
#                                     site  == "Yzerfontein" ~ "18.15731",
#                                     site  == "Kommetjie" ~ "18.32674",
#                                     site  == "Lamberts Bay" ~ "18.30262",
#                                     site  == "Sea Point" ~ "18.38217"))
# 
# test_final <- test %>% 
#   select(date,lat,lon,dir_circ, mean_speed) %>% 
#   dplyr::rename(speed = mean_speed) %>% 
#   dplyr::rename(dir = dir_circ)

# Loading the wind data
library(tidyverse)
library(circular)

# wind_data <- read_csv("wind_data.csv", col_types = cols(X1 = col_number(), date = col_character(), dir = col_number(), hour = col_number(),  speed = col_number())) # Dataset too large for Github but better data at line 638

# wind_data <- read_csv("Data_P1/wind_data.csv")
selected_sites <- c("Port Nolloth", "Lamberts Bay", "Sea Point", "Saldanha Bay", "Hout Bay") 

wind_daily <- wind_data %>%
  # ungroup() %>%
  # mutate(date = as.Date(date))%>%
  dplyr::group_by(temp_sites, date) %>%
  filter(temp_sites %in% selected_sites) %>% 
  dplyr::summarise(dir_circ = round(mean.circular(circular(dir, units = "degrees")),2),
                   mean_speed = round(mean(speed),2)) 


# save(wind_daily, file = "Data/wind_daily.RData")
# # Loading in the daily wind data

# Loading the subsetted wind data created from the text files above 
load("Data/wind_daily.RData") 
```

# Upwelling

Upwelling is primarily caused by alongshore, equator ward winds. These winds are caused by cross-shore atmospheric pressure gradients, and these gradients occur predominantly during heating periods. Upwelling is defined as the process whereby cold, nutrient rich, high concentrated CO2, low pH, and low oxygenated waters are pushed to the surface as a result of alongshore winds interacting with the earth’s rotation.

# Upwelling indeces
  # Determining upwelling index from wind data (SAWS)
  # Index Equation from Fielding & Davis 1989 paper

$$ UpwellingIndex = μ{(Cosθ − 160)}$$

In this equation μ represents the wind speed (m/s) and θ represents the wind direction which is measured in degrees. The 160 degrees is used as this refers to the angle of the coastline. This equation is largely dependant on wind speed and direction data in order to determing the intensity of the upwelling event. Wind data were obtained daily from the South African Weather Service (SAWS). This wind data was then matched to the date at which temperature were collected. With this data the upwelling index was determined.

# Steps
  - Use the circular function
  - Determine daily wind data obtained from 3hr intervals

```{r}
# wind_1 <- read.delim("Data/Wind_data/wind_data.txt(SAWS)/wind1/wind1.txt", na.strings = "",
#                      col.names = c("station_number", "station_name", "date", "hour", "sub", "speed", "dir"))
# 
# wind_2 <- read.delim("Data/Wind_data/wind_data.txt(SAWS)/wind2/wind2.txt", na.strings = "",
#                      col.names = c("station_number", "station_name", "date", "hour", "sub" ,"speed", "dir"))
# 
# wind_3 <- read.delim("Data/Wind_data/wind_data.txt(SAWS)/wind3/wind3.txt", na.strings = "",
#                      col.names = c("station_number", "station_name", "date", "hour", "sub" ,"speed", "dir"))
# 
# # Slecting the important columns for each of the datasets
# wind_fix <- function(df){
# wind <- df %>%
#   select(station_name, date, hour, dir, speed) %>%  #column names
#   mutate(date = as.Date(as.character(date)),
#          hour = as.numeric(as.character(hour)),
#          dir = as.numeric(as.character(dir)),
#          speed = as.numeric(as.character(speed)))
# }
# 
# wind_1 <- wind_fix(df = wind_1)
# wind_2 <- wind_fix(df = wind_2)
# wind_3 <- wind_fix(df = wind_3)
# 
# ## Renaming the sites within the wind datasets to match the name of the sites at which seawater temperature was collected
# ## The wind data was obtained from the SAWS and the wind stations used were the closes stations to which temperature was collected
# 
# renaming_sites_1 <- function(df) {
#   sites <- df %>%
#       mutate(temp_sites = case_when(station_name == "CAPE TOWN TABLE BAY" ~ "Sea Point",
#                                     station_name == "CAPE TOWN - ROYAL YACHT CLUB" ~ "Sea Point",
#                                     station_name  == "PORT NOLLOTH" ~"Port Nolloth",
#                                     station_name  == "CAPE TOWN SLANGKOP" ~ "Hout Bay",
#                                     station_name  == "LAMBERTSBAAI NORTIER" ~ "Lamberts Bay",
#                                     station_name  == "LANGEBAANWEG AWS" ~ "Saldanha Bay"))
#   return(sites)
# }
# 
# 
# wind_sitesmatched_1 <-  renaming_sites_1(df = wind_1)
# wind_sitesmatched_2 <-  renaming_sites_1(df = wind_2)
# wind_sitesmatched_3 <-  renaming_sites_1(df = wind_3)
# wind_data <- rbind(wind_sitesmatched_3,wind_sitesmatched_2,wind_sitesmatched_1)
# save(wind_data, file = "Data/wind_data.RData")
# 
# load("Data/wind_data.RData")
# selected_sites <- c("Port Nolloth", "Lamberts Bay", "Sea Point", "Saldanha Bay", "Hout Bay")
# 
# wind_daily <- wind_data %>%
#   # ungroup() %>%
#   # mutate(date = as.Date(date))%>% # Changing the date to date format
#   dplyr::group_by(temp_sites, date) %>%
#   filter(temp_sites %in% selected_sites) %>%  # Matching the sites to wind temp sites
#   dplyr::summarise(dir_circ = round(mean.circular(circular(dir, units = "degrees")),2), # Direction using the circular function
#                    mean_speed = round(mean(speed),2)) # Calculating the mean
# save(wind_daily, file = "Data/wind_daily.RData")
```

# Load wind

```{r}
load("Data/wind_daily.RData") 
wind_daily <- wind_daily %>% 
  dplyr::rename(sites = temp_sites)
load("Data/site_list_sub.Rdata")
```

# Determining the angle perpendicular to the coastline and plotting it

```{r}
# Coast R- function that calculates the angle of the site perpendicular to the coastline
# Transect function in coastR package finds the angle away or along the coastline
south_africa_away_wide <- coastR::transects(site_list_sub, spread = 30)

# Plotting the map
world_map <- ggplot() + 
  borders(fill = "grey40", colour = "black")


# Create titles
titles <- c("Shore-normal", "Islands")
# Plotting function
plot_sites <- function(site_list, buffer, title_choice, dist){
  
  # Find the point 200 km from the site manually to pass to ggplot
  heading2 <- data.frame(geosphere::destPoint(p = select(site_list, lon, lat),  
                                              b = site_list$heading, d = dist))
  
  # Add the new coordinates tot he site list
  site_list <- site_list %>% 
    mutate(lon_dest = heading2$lon,
           lat_dest = heading2$lat)
  
  # Visualise
  world_map +
    geom_segment(data = site_list, colour = "red4", 
                 aes(x = lon, y = lat, xend = lon_dest, yend = lat_dest)) +
    geom_point(data = site_list, size = 3, colour = "black", aes(x = lon, y = lat)) +
    geom_point(data = site_list, size = 3, colour = "red", aes(x = lon_dest, y = lat_dest)) +
    coord_cartesian(xlim = c(min(site_list$lon - buffer), 
                             max(site_list$lon + buffer)),
                    ylim = c(min(site_list$lat - buffer), 
                             max(site_list$lat + buffer))) +
    labs(x = "", y = "", colour = "Site\norder") +
    ggtitle(titles[title_choice])
}
south_africa_away <- plot_sites(south_africa_away_wide, 1, 1, 100000)
south_africa_away
```

# Upwelling indeces using the above formula

```{r}
UI_angle <- wind_daily %>%  # Making refeerence to the wind daily datta created above
  dplyr::rename(site = sites) %>%  # Renaming sites to site
  left_join(south_africa_away_wide[,c(2,21)], by = "site") %>%  # Joining wind data to south africa wide data column 2 and 21. by site 
  dplyr::rename(coast_angle = heading) %>% # Renamingthe column heading to coast_angle
  mutate(ui.saws = mean_speed * (cos(dir_circ - coast_angle))) %>% # applying the ui formula
  drop_na %>% # removing the na values
  dplyr::rename(t = date) #renaming date to t



# save(UI_angle, file = "Data/UI_angle.RData")
# index <- wind_UI %>%
#   group_by(site) %>%
#   select("date", "ui.saws") %>%
#   mutate(saws.condition = ifelse(ui.saws < 12.5, "upwelling"))
# wind_UI <- wind_daily %>% 
#   mutate(ui.saws = mean_speed * (cos(dir_circ - 160))) %>% # 160 is angle perpendicular to the west coast of SA (See paper Fielding and Davis pg 182)
#   drop_na
# Results obtained from UI_angle (Using the perpendicular angle for each site) and the results of wind_UI at -160 (angle of the west coast) appears to be the same
```

# Exceedence function heatwaveR - This is done to determine the consecutive number of days at or above what the UI value is meant to be   # Detect consecutive days in exceedance of a given threshold.

```{r}
# Loading the insitu temperature data along the wc
load("Data/SACTN_US.RData") # temperature data for all the sites with a 30year time series
# exceedance <-
#   function(data,
#            x = t,
#            y = temp,
#            threshold,
#            below = FALSE,
#            minDuration = 5,
#            joinAcrossGaps = TRUE,
#            maxGap = 2,
#            maxPadLength = 3)
# Upwelling temperature threshold 12.4 / 30th percentile
SACTN_upwell <- UI_angle %>% 
  dplyr::rename(temp = ui.saws) %>%
  group_by(site) %>%
  # mutate(min_t = min(t), 
  #        max_t = max(t)) %>% 
  nest() %>% # apply the following functions to all of the variables in te dataset
  mutate(clim = purrr::map(data, ts2clm, climatologyPeriod = c("2000-01-01", "2005-12-31")), # creating a column of climaatologies. Column will be named clim
         # NB: A threshold of 3 appeared to be far to strict
         # purr::map - apllies a function to each element of a vector
         exceed = purrr::map(clim, exceedance, minDuration = 1, threshold = 1)) %>%  #Upwelling cannot be descrbed as an event. Upwelling can last for a few hours. Given that we have daily data, upwelling events minimum duration here will be 1day
  # Detect consecutive days in exceedance of a given threshold.
  # mutate() %>% 
  select(-data, -clim) %>% 
  unnest() %>%
  filter(row_number() %% 2 == 1) %>%
  unnest() %>% # creates a column for each variables
  dplyr::rename(ui.saws = temp) %>% # rename upwelling index vale to temp so that it could work with the function
  select(site, t, ui.saws, exceedance) # selecting only these variables
# Calculate quantiles of upwelling index

# Static numbers are often rejected and so we decided to find a percentile value as these are often more likely to be approved

SACTN_upwell_quantiles <- SACTN_upwell %>% 
  filter(ui.saws >= 0) %>% # Upwelling occurs for all values above 0. Values below this is regarded as downwelling
  group_by(site) %>% 
  summarize(quant_10 = quantile(ui.saws, probs = 0.10, na.rm = TRUE),
            quant_25 = quantile(ui.saws, probs = 0.25, na.rm = TRUE),
            quant_50 = quantile(ui.saws, probs = 0.50, na.rm = TRUE),
            quant_75 = quantile(ui.saws, probs = 0.75, na.rm = TRUE),
            quant_90 = quantile(ui.saws, probs = 0.90, na.rm = TRUE)) %>% 
  mutate_if(is.numeric, round, digits = 2)

# SACTN_upwell_transform <- as.data.frame(read.zoo(transform( SACTN_upwell, Date = as.POSIXct(t) ), FUN = identity ))
# There are a few ideas on how to go about doing this
# The first is to calculatea rolling climatology and to check the upwelling phenology agianst that
# in order to determine changes in upwelling values, but this won't give us the metrics we want
# Rather what we can do is set a static bottom threshold below which we are interested in looking for upwelling
# We then use the upwelling index values created above as a second filter to determine the metrics of the upwelling
# Unfortunately the way that the heatwaveR functions work well does with purrr. 
# I've figured this out once before but can't remember where now.
# Rather than go through the process I have rather just made a convenience function below that
# does what I want and can simply be given to purrr::map() without any faffing about.


# Detect event: 
detect_event_custom <- function(df){
  res <- detect_event(df, threshClim2 = df$exceedance, minDuration = 3, coldSpells = T)
  return(res)
}
# Calculate the upwelling event metrics
SACTN_upwell_base <- SACTN_US %>% 
  dplyr::rename(t = date) %>% 
  group_by(site) %>% 
  nest() %>% 
  # The climatology base period used here is up for debate...
  # The choice of the 30th percentile threshold also needs to be justified and sensitivty tested
  mutate(clim = purrr::map(data, ts2clm, pctile = 25, climatologyPeriod = c("1995-01-01", "2004-12-31"))) %>%
  select(-data) %>% 
  unnest() %>%
  left_join(SACTN_upwell, by = c("site", "t")) %>%
  filter(!is.na(exceedance)) %>%
  group_by(site) %>% 
  # mutate(thresh = mean(seas, na.rm = T)-sd(seas, na.rm = T)) %>% # Manually set threshold to the mean
  # mutate(thresh = 10) %>% # Manually set threshold to a static value. Doesn't work across all sites
  # mutate(thresh = quantile(temp, 0.3, na.rm = T)) %>% # Manually set threshold to a single quantile
  nest() %>% 
  mutate(exceed = purrr::map(data, detect_event_custom)) %>% 
  select(-data) #%>% 
  # unnest() %>% 
  # filter(row_number() %% 2 == 0) %>% # Select event summary metrics
  # filter(row_number() %% 2 == 1) %>% # Select daily values
  # unnest()
# Unpack the event metric reults
SACTN_upwell_events <- SACTN_upwell_base %>% 
  unnest() %>%
  filter(row_number() %% 2 == 0) %>%
  unnest()
# save(SACTN_upwell_events, file = "Data/SACTN_upwell_events.RData")
# Unpack the daily climatology results
SACTN_upwell_clims <- SACTN_upwell_base %>% 
  unnest() %>%
  filter(row_number() %% 2 == 1) %>% 
  unnest()
# save(SACTN_upwell_clims, file = "Data/SACTN_upwell_clims.RData")
# The above chunk of code appears to work as expected.
# What needs to be done now is that the thresholds for duration and UI strength need to be justified/decided uopn. 
    # Min duration for an upwelling event - 1day and threshold -1
# What also needs to be decided is if we are interested in any upwelling results throughout the year,
# or only during the upwelling season, whenever that may be for each site.
# I think we have demonstrated the technical capability needed to answer the question of whether or not upwelling
# is changing over time.
# Now we really need to focus on which of these parameters need to best be tweaked to answer that question.
```


# Loading in all the data created using the code bellow

```{r}
load("Data/site_list_sub.Rdata")
load("Data/SACTN_US.RData")
load("Data/site_pixels.RData") # 5 decimal places
load("Data/OISST.RData") # 2 decimal places
OISST <- BC_avhrr_only_v2_Document_Document 
rm(BC_avhrr_only_v2_Document_Document ); gc()
load("Data/CMC.RData") # 1decimal places
```


 Using the upwelling metrics created in `upwell.IDX.Rmd` Identify when upwelling occurs at the particular site. 
# Is this upwelling event seen throughout the different distances from the coastline
# Using CMC, MUR, OISST and SACTN_US to determine wether the upwelling events detected are present in each of the datasets

## Function
Determining the temperatures at the various distances from the coastline

```{r}
# # These following three objects (MUR, OISST, CMC) need to be the complete set of lon/lat values for your satellite data
# # MUR <- MUR %>% 
# #   select(lon, lat) %>% 
# #   mutate(product = "MUR")
# 
# # Decided to work with OISST and CMC as both has a time series of 30years
# OISST_prod <- OISST %>% 
#   select(lon, lat) %>% 
#   unique() %>% 
#   mutate(product = "OISST")
# 
# CMC_prod <- CMC %>% 
#   select(lon, lat) %>%
#   unique() %>% 
#   mutate(product = "CMC")
# 
# # sat_data <- rbind(CMC_prod, OISST_prod) %>% 
#   #rbind(., CMC) %>% 
#   # select(product, lon, lat)
# 
# # sat_pixels <- sat_data %>% 
#   # select(product, lon, lat) %>%
#   # unique()
# 
# ## For testing the nest/map pipeline
# # df <- site_pixels %>%
# #   filter(site == "Lamberts Bay") %>%
# #   select(-site)
# 
# match_func <- function(df){
#   df <- df %>%
#     dplyr::rename(lon_site = lon, lat_site = lat)
#   OISST_index <- OISST_prod[as.vector(knnx.index(as.matrix(OISST_prod[,c("lon", "lat")]),
#                                                  as.matrix(df[,c("lon_site", "lat_site")]), k = 1)),] %>% 
#     cbind(., df)
#   CMC_index <- CMC_prod[as.vector(knnx.index(as.matrix(CMC_prod[,c("lon", "lat")]),
#                                              as.matrix(df[,c("lon_site", "lat_site")]), k = 1)),] %>% 
#     cbind(., df)
#   res <- rbind(OISST_index, CMC_index)
#   return(res)
# }
# 
# # Find the nearest pixels to each spot along the transect
# # NB: SOme pixels are used more thanonce in a transect as the spacing isn't quite larger enough between transect
# # points to not fall inside of the same 25 KM pixel
# pixel_match <- site_pixels %>% 
#   group_by(site) %>% 
#   group_modify(~match_func(.x))
# 
# # You may then use the 'pixel_match' object to filter out the desired pixels from the full satellite products
# OISST_fill <- right_join(OISST, filter(pixel_match, product == "OISST"), by = c("lon", "lat"))
# CMC_fill <- right_join(CMC, filter(pixel_match, product == "CMC"), by = c("lon", "lat"))
# #SACTN_fill <- right_join(SACTN, filter(pixel_match, product == "SACTN"), by = c("lon", "lat"))
# 
# # Clean up some RAM space
# rm(OISST, CMC); gc()
# 
# # sites <- c("Port Nolloth", "Lamberts Bay", "Saldanha Bay", "Sea Point")
# # # In the SACTN dataset Hout Bay only has data until 2005. Hout Bay will now be removed from this study
# # # Hout Bay will be ignored
# selected_sites <- c("Port Nolloth", "Lamberts Bay", "Sea Point", "Saldanha Bay")
# 
# OISST_fill <- OISST_fill %>%
#   filter(site %in% selected_sites)
# 
# CMC_fill <- CMC_fill %>%
#   filter(site %in% selected_sites)
# 
# 
# # All the code below can be viewed in upwell_IDX.Rmd
# # Next to run upwelling index
# # UI Created using SAWS wind data (Find in upwell_IDX_Rmd)
# load("Data/UI_angle.RData")
# 
# upwelling <- UI_angle %>% 
#   dplyr::rename(temp = ui.saws) %>%
#   group_by(site) %>%
#   # mutate(min_t = min(t), 
#   #        max_t = max(t)) %>% 
#   nest() %>% # apply the following functions to all of the variables in te dataset
#   mutate(clim = purrr::map(data, ts2clm, climatologyPeriod = c("1997-01-01", "2015-12-31")), # creating a column of climatologies. Column will be named clim
#          # NB: A threshold of 3 appeared to be far to strict
#          # purr::map - apllies a function to each element of a vector
#          exceed = purrr::map(clim, exceedance, minDuration = 1, threshold = 1)) %>%  #Upwelling cannot be descrbed as an event. Upwelling can last for a few hours. Given that we have daily data, upwelling events minimum duration here will be 1day
#   # Detect consecutive days in exceedance of a given threshold.
#   # mutate() %>% 
#   select(-data, -clim) %>% 
#   unnest() %>%
#   filter(row_number() %% 2 == 1) %>%
#   unnest() %>% # creates a column for each variables
#   dplyr::rename(ui.saws = temp) %>% # rename upwelling index vale to temp so that it could work with the function
#   select(site, t, ui.saws, exceedance) 
# 
# # Now applying the Upwelling func
# # NB: This only pulls out the event results and not the climatology results
# # This is done to keep the output tidy because group_modify() may only create data.frame type outputs
# # To create a list output one would use group_map(), 
# # but this then loses the labels of which sites etc. the results belong to
# detect_event_custom <- function(df){
#   res <- detect_event(df, threshClim2 = df$exceedance, minDuration = 3, coldSpells = T)$event
#   return(res)
# }
# 
# ts2clm_custom <- function(df){
#   # The climatology base period used here is up for debate...
#   # The choice of the 25th percentile threshold also needs to be justified and sensitivty tested
#   res <- ts2clm(df, pctile = 25, climatologyPeriod = c("1992-01-01", "2016-12-31"))
#   return(res)
# }
# 
# # Calculate the upwelling event metrics
# upwelling_detect_event <- function(df){
#   upwell_base <- df %>% 
#     dplyr::rename(t = date) %>% 
#     group_by(site, product, heading, distance, lon, lat) %>% 
#     group_modify(~ts2clm_custom(.x)) %>% 
#     left_join(upwelling, by = c("site", "t")) %>%
#     filter(!is.na(exceedance)) %>%
#     group_by(site, product, heading, distance, lon, lat) %>% 
#     group_modify(~detect_event_custom(.x))
#   }
# 
# OISST_upwell_base <- upwelling_detect_event(df = OISST_fill)
# save(OISST_upwell_base, file = "Data/OISST_upwell_base.RData")
# CMC_upwell_base <- upwelling_detect_event(df = CMC_fill)
# save(CMC_upwell_base, file = "Data/CMC_upwell_base.RData")
# 
# # Here we remove the site Hout Bay so that we have a long time series. The length of Hout Bay time series ends in 200. Many sites change from here
# SACTN_US <- SACTN_US %>%
#   filter(site %in% selected_sites)
# 
# load("Data/site_list_v4.2.RData")
# 
# SACTN_upwell_base <- SACTN_US %>%
#   left_join(site_list[,c(4, 5, 6)], by = "index")%>% 
#     dplyr::rename(t = date) %>% 
#     group_by(site, lon, lat) %>% 
#     group_modify(~ts2clm_custom(.x)) %>% 
#     left_join(upwelling, by = c("site", "t")) %>%
#     filter(!is.na(exceedance)) %>%
#     group_by(site, lon, lat) %>% 
#     group_modify(~detect_event_custom(.x))
# 
# save(SACTN_upwell_base, file = "Data/SACTN_upwell_base.RData")
```

# PLotting events

Loading the data
```{r}
load("Data/OISST_upwell_base.RData")
load("Data/CMC_upwell_base.RData")
load("Data/SACTN_upwell_base.RData")
```

To accurately compare 
  Start date CMC start date 1991-09-02 end date 2017-03-15
  Start date OISST start date 1981-09- 01  and end date 2018-09-28
  Start date SACTN start date 1973 and end date 2017-12-16
  
```{r}

lm_coeff <- function(df){
  res <- lm(formula = val ~ date_peak, data = df)
  res_coeff <- round(as.numeric(res$coefficient[2]), 4)
}
# Changes in upwelling metrics
lm_func <- function(df){
  upwell_lm <- df %>% 
  select(-c(index_start:index_end)) %>% 
  gather(key = "var", value = "val", -c(site:date_end)) %>% 
  group_by(site, var, distance) %>% 
  nest() %>% 
  mutate(slope = purrr::map(data, lm_coeff)) %>% 
  select(-data) %>% 
  unnest() %>% 
  # convert from daily to decadal values
  mutate(slope = slope * 365.25*10)
}

OISST_lm <- lm_func(df = OISST_upwell_base)
CMC_lm <- lm_func(df = CMC_upwell_base)

SACTN_lm <- SACTN_upwell_base %>% 
  select(-c(index_start:index_end)) %>% 
  gather(key = "var", value = "val", -c(site:date_end)) %>% 
  group_by(site, var) %>% 
  nest() %>% 
  mutate(slope = purrr::map(data, lm_coeff)) %>% 
  select(-data) %>% 
  unnest() %>% 
  # convert from daily to decadal values
  mutate(slope = slope * 365.25*10)
```

# ANOVA
- Relationship between duration, year and distance as a function of the mean intensity. 
```{r}
options(scipen = 999)
anova_func <- function(df){
  sites_aov <- aov(intensity_mean ~ site * duration * distance, data = df)
return(sites_aov)
}

OISST_anov <- anova_func(df = OISST_upwell_base)
summary(OISST_anov)
CMC_anov <- anova_func(df = CMC_upwell_base)
summary(CMC_anov)

SACTN_anov <- sites_aov <- aov(intensity_mean ~ site * duration, data = SACTN_upwell_base)
summary(SACTN_anov)
```

# Wind plots

```{r, fig.cap = "Windrose diagram representing the wind direction and speed for each of the sites", fig.height = 10, fig.width = 15}
library(ggradar)
library(dplyr)
library(scales)
library(tibble)
load("Data/UI_angle.RData")

# Detects NA values, no NA values are present
# Plotting wind variables
source("Functions/wind.rose.R") # This it may be related to the countmax = NA in the function

wind_daily_renamed <- UI_angle %>% 
  mutate(dir_circ = ifelse(dir_circ < 0, dir_circ+360, dir_circ)) %>% 
  dplyr::rename(spd = mean_speed) %>%
  dplyr::rename(dir = dir_circ) %>% 
  filter(spd > 0)

p.wr2 <- plot.windrose(data = wind_daily_renamed,
              spd = "spd",
              dir = "dir")

p.wr3 <- p.wr2 + facet_wrap(.~ site, ncol = 2, nrow = 2) +
  theme(strip.text.x = element_text(size = 25)) + theme(panel.spacing = unit(2, "lines"))
p.wr3
```

# Lolli plot

```{r}
plot_loli_func <- function(df){
  ggplot(df, aes(x = date_peak, y = duration)) + 
  geom_lolli(aes(colour = intensity_cumulative)) + 
  scale_color_distiller(palette = "Spectral", name = "Cumulative \nintensity") + 
  facet_wrap(~distance, ncol = 2) +
  xlab("Date") + ylab("Event duration [days]") 
}

(OISST_SP_pl <- plot_loli_func(df = OISST_SEAPOINT))
(OISST_SB_pl <- plot_loli_func(df = OISST_SB))
(OISST_PN_pl <- plot_loli_func(df = OISST_PN))
(OISST_LB_pl <- plot_loli_func(df = OISST_LB))
OISST_SP_pl <- OISST_SP_pl +
  ggtitle("Sea Point")
OISST_SB_pl <- OISST_SB_pl +
  ggtitle("Saldanha Bay")
OISST_PN_pl <- OISST_PN_pl +
  ggtitle("Port Nolloth")
OISST_LB_pl <- OISST_LB_pl +
  ggtitle("Lamberts Bay")

combined_OISST_pl <- ggarrange(OISST_SP_pl, OISST_SB_pl, OISST_PN_pl,OISST_LB_pl)

#######
# CMC

(CMC_SP_pl <- plot_loli_func(df = CMC_SEAPOINT))
(CMC_SB_pl <- plot_loli_func(df = CMC_SB))
(CMC_PN_pl <- plot_loli_func(df = CMC_PN))
(CMC_LB_pl <- plot_loli_func(df = CMC_LB))

CMC_SP_pl <- CMC_SP_pl +
  ggtitle("Sea Point")
CMC_SB_pl <- CMC_SB_pl +
  ggtitle("Saldanha Bay")
CMC_PN_pl <- CMC_PN_pl +
  ggtitle("Port Nolloth")
CMC_LB_pl <- CMC_LB_pl +
  ggtitle("Lamberts Bay")

combined_CMC_pl <- ggarrange(CMC_SP_pl, CMC_SB_pl, CMC_PN_pl,CMC_LB_pl)

plot_loli_func <- function(df){
  ggplot(df, aes(x = date_peak, y = duration)) + 
  geom_lolli(aes(colour = intensity_cumulative)) + 
  scale_color_distiller(palette = "Spectral", name = "Cumulative \nintensity") + 
  facet_wrap(~site, ncol = 2) +
  xlab("Date") + ylab("Event duration [days]") 
}
(SACTN_plot <- plot_loli_func(df = SACTN_filtered))
```

