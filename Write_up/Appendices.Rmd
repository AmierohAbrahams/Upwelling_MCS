---
title: Quantifying the impact of wind and wave action on seawater temperature along the South African coastline (Appendices)
author:
- affiliation: University of the Western Cape
  name: Amieroh Abrahams
date: "19 November 2018"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 6.5
    fig_width: 6.5
    highlight: default
    keep_tex: yes
    latex_engine: xelatex
    template: ../LaTeX/template-article_ajs.tex
  html_document:
    fig_caption: yes
    fig_height: 7
    fig_retina: 2
    fig_width: 7
    highlight: espresso
    theme: cerulean
    toc_float: no
  md_document:
    variant: markdown_github
  word_document:
    fig_caption: yes
    fig_height: 7
    fig_width: 7
    highlight: zenburn
    toc: no
fontsize: 10pt
geometry: margin=1in
language: Australian
csl: ../LaTeX/frontiers.csl
papersize: A4
tables: yes
bibliography: ../LaTeX/appendices.bib
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.path = 'compliled_figures/', include = TRUE, 
                      echo = TRUE, warning = FALSE, message = FALSE, 
                      tidy = FALSE, width.cutoff = 80, cache = FALSE, 
                      results = "hide", size = "small")
```

This file was generated in R using Rmarkdown, with a bit of \LaTeX\ thrown in:
```{r include = TRUE, results='markup'}
sessionInfo()
```

\newpage

\section*{\large{Appendix A}}

\subsection*{Background and code}

The intention of this section is to show the approach and **R** scripts used to analyse variation in upwelling events at different distances from the coastline within the Benguela upwelling system. Specifically, I wish to determine if the same upwelling signals are present throughout the different distances along the coastline and if the same signals are present within the different datasets.

\subsubsection*{The data}
The SACTN dataset is the primary source of temperature data used in this study. This dataset consisted of *in situ* collected coastal seawater temperatures for 129 sites along the coast of South Africa, measured daily from 1972 until 2017. This study also make use of AVHRR OISST and CMC satellite-derived SST datasets as well as wind action data that were obtained from the South African Weather Service (SAWS), and were provided at three hour resolutions.

\subsubsection*{Setting up the analysis}
This is **R**, so first I need to find, install and load various packages. Some of the packages will be available on CRAN and can be accessed and installed in the usual way.

```{r prelim_opts, echo=FALSE}
knitr::opts_chunk$set(
  comment = "R>",
  warning = FALSE,
  message = FALSE 
)


# Disable scientific notation for numeric values
# I just find it annoying
options(scipen = 999)

library(tidyverse)
library(lubridate)
library(ggpubr); theme_set(theme_pubr())
library(zoo)
library(data.table)
library(heatwaveR)
library(viridis)
library(gridExtra)
library(fossil)
library(stringr)
library(doMC); doMC::registerDoMC(cores = 4)
library(fasttime)
library(xtable)
library(plyr)
library(ggpubr)
library(zoo)
library(FNN)
library(forecast)
library(astrochron)
library(WaveletComp)
library(data.table)
library(viridis)
library(ggrepel)
library(plyr)
library(maptools)
library(sp)
library(geosphere)
library(PBSmapping)
library(scales)
library(grid)
library(gridExtra)
library(fossil)
library(mapproj)
library(ncdf4)
library(reshape2)
library(gridExtra)
library(geosphere)
library(circular)
#library(plyr) # Never load plyr when also loading the tidyverse. It causes a lot of conflicts.
# devtools::install_github("robwschlegel/coastR")

# install_github("marchtaylor/sinkr")

## Functions
source("../Functions/earthdist.R")
source("../Functions/wind.rose.R")
source("../Functions/theme.R")
source("../Functions/scale.bar.func.R")
```

\newpage

\section*{\large{Appendix B}}

Now to get to the data. Here I load the site list dataset. This dataset comprise of the statistical properties of the seawater temperature representing the South African coastline, such as the mean, minimum and maximum temperatures. These values vary among coastal sections due to the influence of the cold Benguala and warm Agulhas currents. At the broad scale, this region exhibits a large variation in seawater temperatures along its coastline [@Mead2013; @Smit2013] and is divided into four bioregions, each with contrasting temperatures. These bioregions are the Benguela Marrine Province (BMP), Benguela-Agulhas Transition Zone (B-ATZ), the Agulhas Marine Province (AMP) and the East Coast Transition Zone (ECTZ) [@Smit2017]. The Benguela Marine Province (BMP) is located to the west of Cape Point and is characterized by the movement of cold water from the Southern Ocean moving north. The cold temperate west coast mentioned above is greatly affected by the upwelling caused by offshore winds, generated by the cold Benguela current. Thus the seawater temperatures found around South Africaâ€™s coastline exhibits a large variational range. My plotting functions partition the data into the clusters and colour code each point accordingly so that I am able to see patterns that exist and distictly group the different sites with similar temperatures along the three distinct coasts. 

```{r load_files1, include=TRUE}
load("../data/site_list_v4.2.RData")
```

\newpage

The next step involves SACTN temperature dataset. First I compute the `filter` function on a data matrix, which only selects the data along the west coast of South Africa (within the Benguela upwelling region) and select all the sites with a time series of at least 30 years of data.

```{r filtered-sites}
site_list_sub <- site_list %>%
  filter(coast == "wc") %>%
  filter(length > 10950) 
#site_list_sub <- site_list_sub[-5,]
save(site_list_sub, file = "Data/site_list_sub.Rdata")

SACTN_US <- SACTN_daily_v4.2 %>%
  left_join(site_list[,c(4,13)], by = "index") %>%
  filter(index %in% site_list_sub$index) %>%
  separate(index, into = c("site", "src"), sep = "/", remove = FALSE)

# save(SACTN_US, file = "Data/SACTN_US.RData") #Saving the file
```

This thermal data contain various variables, in this study I only make use of some of them. This resulted in a total of 5 sites. Later in the analyses I however selected only 4 of the five sites as they represented better overlapping years.

<!-- \def\tiny{\@setfontsize\tiny{10pt}{11pt}} -->
\begin{tiny}
\begin{center}
\setlength\tabcolsep{3pt}
\begin{longtable}{|r|r|r|l|p{3cm}|}
\caption{The 4 sites located within the Benguela, with approximate GPS coordinates and source} \\

% This is the header for the first page of the table...
\toprule
 Site & SRC & Lon & Lat  \\
\midrule
\endfirsthead

% This is the header for the second page of the table...
\toprule
Site & SRC & Lon & Lat  \\
\midrule
\endhead

% This is the footer for all pages except the last page of the table...
\midrule
\multicolumn{5}{l}{{Continued on Next Page\ldots}} \\
\endfoot

% This is the footer for the last page of the table...
\bottomrule
\endlastfoot

% Now the data...
Saldanha Bay & SAWS & 17.95 & -33.01 \\
Port Nolloth & SAWS & 16.87 & -29.25 \\
Lamberts Bay & SAWS & 18.30 & -32.09 \\
Sea Point & SAWS & 18.38 & -33.92 \\

\end{longtable}
\end{center}
\end{tiny}

```{r load_files2, include=TRUE}
load("../Data/site_list_sub.Rdata")
load("../Data/africa_coast.RData")
xtable(site_list_sub, auto = TRUE)
west <- site_list_sub[-5,]
west$coast <- "west" # Chnages wc to west

site_list_sub <- site_list_sub[-5,] # Removing Hout Bay from the site list

## Downloading the bathy data from NOAA
# Download mid-res bathymetry data
# sa_lat <- c(-38, -24.5); sa_lon <- c(11.5, 35.5)
# sa_bathy <- as.xyz(getNOAA.bathy(lon1 = sa_lon[1], lon2 = sa_lon[2], lat1 = sa_lat[1], lat2 = sa_lat[2], resolution = 4))
# colnames(sa_bathy) <- c("lon", "lat", "depth")
# sa_bathy <- sa_bathy[sa_bathy$depth <= 0,]
# save(sa_bathy, file = "Data_P1/bathy/sa_bathy.RData")

# Loading in the newly downloaded bathymetry data
load("../Data/sa_bathy.RData")
load("../Data/africa_coast.RData")
```


Thereafter, inorder to determine the variation of upwelling events at different distances from the coastline it was important to first obtain the coordinates for the different distances. The function produces a dataframe for coordinates at the different distances from the coastline for each of the sites. The coordinates were obtained at 10000km, 20000km, 30000km, 40000km and 50000km respectively. 

```{r shorenormal-func}
# This function takes one site (e.g. one set of lon/lats) and calculates a shore normal transect
shore.normal.transect <- function(site, width = 2){
  # Find the site on the coastline and it's nearest neighbour points
  coords <- data.frame(lon = site$lon, lat = site$lat)
  coords2 <- knnx.index(africa_coast[,1:2], as.matrix(coords), k = 1)
  coords3 <- data.frame(site = site$site, africa_coast[c(coords2-width, coords2+width),]) 
  coords3 <- coords3[2:1,1:3]
  # Define the shore normal transect bearing
  heading <- earth.bear(coords3[1,2], coords3[1,3], coords3[2,2], coords3[2,3]) + 90
  if(heading >= 360){
    heading <- heading-360
  } else {
    heading <- heading
  }
  heading2 <- data.frame(site = site$site, lon = site$lon, lat = site$lat, heading)
  return(heading2)
}

# Creating the transects
site_transects <- data.frame()
for(i in 1:length(west$site)){
 site <- west[i,]
 site_transect <- shore.normal.transect(site, 2)
 site_transects <- rbind(site_transects, site_transect)
}

# Manually correcting Sea Point and Kommetjie
site_transects$heading[4] <- 290 
# save(site_transects, file = "Data/site_transects.RData")
load("../Data/site_transects.RData")

# This function takes one site (e.g. one set of lon/lats) and calculates a shore normal transect
# It then extracts a lat/ lon point every X kilometres until reaching a specified isobath

transect.pixel <- function(site, distances){
  # Extract coordinates
  coords <- data.frame(lon = site$lon, lat = site$lat)
  # Find lon/ lats every X metres 
  pixels <- data.frame()
  # deep <- 999
  # distance_multiplier <- 1
  # while(deep > isobath){
  for(i in 1:length(distances)){
    coords2 <- as.data.frame(destPoint(p = coords, b = site$heading, d = distances[i]))
    sitesIdx <- knnx.index(sa_bathy[,1:2], as.matrix(coords2), k = 1)
    bathy2 <- sa_bathy[sitesIdx,]
    bathy2 <- bathy2[complete.cases(bathy2$depth),]
    bathy3 <- data.frame(site = site$site, lon = bathy2$lon, lat = bathy2$lat, 
                         heading = site$heading, 
                         distance = distances[i])
    pixels <- rbind(pixels, bathy3)
    coords <- coords2
  }
  if(nrow(pixels) < 1){
    pixels <- data.frame(site, depth = NA)
  }else{
    pixels <- pixels
  }
  return(pixels)
}

# Pixel points
site_pixels <- data.frame()
for(i in 1:length(west$site)){
  site <- site_transects[i,]
  site_pixel <- transect.pixel(site, c(10000, 20000, 30000, 40000, 50000)) # RWS: fixed error
  site_pixels <- rbind(site_pixels, site_pixel)
}

# save(site_pixels, file = "Data/site_pixels.RData")
load("../Data/site_pixels.RData")

# Bounding box
  # Only one is made in order to know how large the the geom_point() squares should be made to match
bbox <- data.frame(xmin = destPoint(p = site_pixels[1,2:3], b = 270, d = 12500)[1],
                   xmax = destPoint(p = site_pixels[1,2:3], b = 90, d = 12500)[1],
                   ymin = destPoint(p = site_pixels[1,2:3], b = 180, d = 12500)[2],
                   ymax = destPoint(p = site_pixels[1,2:3], b = 0, d = 12500)[2])

# Determining the temperature at the various distances from the coast
```

Here I created a chunck of code to load in the wind data provided by SAWS. This data is presented in single text files and as a result I will not be adding the code here. As mentioned earlier, this code is present in my Github repository.  The wind direction was collected every 2 hours and by using the circular function we calcuated the daily wind direction and mean speed. There is a numerically large gap between 360 and 2 where as in degrees its not as large. The circular mean function returns the mean direction of a vector of circular data. https://cran.r-project.org/web/packages/circular/circular.pdf (Pg 130). The circular function creates circular objects around the wind direction. 

```{r load-winddata}
wind_data <- read_csv("../Data_P1/wind_data.csv")
selected_sites <- c("Port Nolloth", "Lamberts Bay", "Sea Point", "Saldanha Bay", "Hout Bay") 

wind_daily <- wind_data %>%
  # ungroup() %>%
  # mutate(date = as.Date(date))%>%
  dplyr::group_by(temp_sites, date) %>%
  filter(temp_sites %in% selected_sites) %>% 
  dplyr::summarise(dir_circ = round(mean.circular(circular(dir, units = "degrees")),2),
                   mean_speed = round(mean(speed),2)) 


# save(wind_daily, file = "Data/wind_daily.RData")
# # Loading in the daily wind data

# Loading the subsetted wind data created from the text files above 
load("../Data/wind_daily.RData") 
```

\subsubsection*{Upwelling Index}

Upwelling is primarily caused by alongshore, equator ward winds. These winds are caused by cross-shore atmospheric pressure gradients, and these gradients occur predominantly during heating periods. Upwelling is defined as the process whereby cold, nutrient rich, high concentrated CO2, low pH, and low oxygenated waters are pushed to the surface as a result of alongshore winds interacting with the earthâ€™s rotation.

$$ UpwellingIndex = Î¼{(CosÎ¸ âˆ’ 160)}$$

In this equation Î¼ represents the wind speed (m/s) and Î¸ represents the wind direction which is measured in degrees. The 160 degrees is used as this refers to the angle of the coastline. This equation is largely dependant on wind speed and direction data in order to determing the intensity of the upwelling event. Wind data were obtained daily from the South African Weather Service (SAWS). This wind data was then matched to the date at which temperature were collected. With this data the upwelling index was determined. Inorder to use this formula I first determined the angle perpendicular to the coastline using the `coastR` function. Thereafter, I determined the upwelling index for each of the sites.

```{r upwellingIndex-application}

south_africa_away_wide <- coastR::transects(site_list_sub, spread = 30)

# # Plotting the map
# world_map <- ggplot() + 
#   borders(fill = "grey40", colour = "black")
# 
# # Create titles
# titles <- c("Shore-normal", "Islands")
# # Plotting function
# plot_sites <- function(site_list, buffer, title_choice, dist){
#   
#   # Find the point 200 km from the site manually to pass to ggplot
#   heading2 <- data.frame(geosphere::destPoint(p = select(site_list, lon, lat),  
#                                               b = site_list$heading, d = dist))
#   
#   # Add the new coordinates to the site list
#   site_list <- site_list %>% 
#     mutate(lon_dest = heading2$lon,
#            lat_dest = heading2$lat)
# 
UI_angle <- wind_daily %>%  # Making refeerence to the wind daily datta created above
  dplyr::rename(site = sites) %>%  # Renaming sites to site
  left_join(south_africa_away_wide[,c(2,21)], by = "site") %>%  # Joining wind data to south africa wide data column 2 and 21. by site
  dplyr::rename(coast_angle = heading) %>% # Renamingthe column heading to coast_angle
  mutate(ui.saws = mean_speed * (cos(dir_circ - coast_angle))) %>% # applying the ui formula
  drop_na %>% # removing the na values
  dplyr::rename(t = date) #renaming date to t
```

Here I make use of the `exceedence` function within the heatwaveR package. This was primarily done to determine the consecutive number of days at or above what the UI value is meant to be. The scipt however may be found in my [GitHub](https://github.com/AmierohAbrahams/Upwelling_MCS) repository, and the full code can be run in its entiety.

```{r upwelling_thresholds}
# Loading the insitu temperature data along the wc
load("Data/SACTN_US.RData") # temperature data for all the sites with a 30year time series
# exceedance <-
#   function(data,
#            x = t,
#            y = temp,
#            threshold,
#            below = FALSE,
#            minDuration = 5,
#            joinAcrossGaps = TRUE,
#            maxGap = 2,
#            maxPadLength = 3)
# Upwelling temperature threshold 12.4 / 30th percentile
SACTN_upwell <- UI_angle %>% 
  dplyr::rename(temp = ui.saws) %>%
  group_by(site) %>%
  # mutate(min_t = min(t), 
  #        max_t = max(t)) %>% 
  nest() %>% # apply the following functions to all of the variables in te dataset
  mutate(clim = purrr::map(data, ts2clm, climatologyPeriod = c("2000-01-01", "2005-12-31")), # creating a column of climaatologies. Column will be named clim
         # NB: A threshold of 3 appeared to be far to strict
         # purr::map - apllies a function to each element of a vector
         exceed = purrr::map(clim, exceedance, minDuration = 1, threshold = 1)) %>%  #Upwelling cannot be descrbed as an event. Upwelling can last for a few hours. Given that we have daily data, upwelling events minimum duration here will be 1day
  # Detect consecutive days in exceedance of a given threshold.
  # mutate() %>% 
  select(-data, -clim) %>% 
  unnest() %>%
  filter(row_number() %% 2 == 1) %>%
  unnest() %>% # creates a column for each variables
  dplyr::rename(ui.saws = temp) %>% # rename upwelling index vale to temp so that it could work with the function
  select(site, t, ui.saws, exceedance) # selecting only these variables
# Calculate quantiles of upwelling index

# Static numbers are often rejected and so we decided to find a percentile value as these are often more likely to be approved

SACTN_upwell_quantiles <- SACTN_upwell %>% 
  filter(ui.saws >= 0) %>% # Upwelling occurs for all values above 0. Values below this is regarded as downwelling
  group_by(site) %>% 
  summarize(quant_10 = quantile(ui.saws, probs = 0.10, na.rm = TRUE),
            quant_25 = quantile(ui.saws, probs = 0.25, na.rm = TRUE),
            quant_50 = quantile(ui.saws, probs = 0.50, na.rm = TRUE),
            quant_75 = quantile(ui.saws, probs = 0.75, na.rm = TRUE),
            quant_90 = quantile(ui.saws, probs = 0.90, na.rm = TRUE)) %>% 
  mutate_if(is.numeric, round, digits = 2)

# SACTN_upwell_transform <- as.data.frame(read.zoo(transform( SACTN_upwell, Date = as.POSIXct(t) ), FUN = identity ))
# There are a few ideas on how to go about doing this
# The first is to calculatea rolling climatology and to check the upwelling phenology agianst that
# in order to determine changes in upwelling values, but this won't give us the metrics we want
# Rather what we can do is set a static bottom threshold below which we are interested in looking for upwelling
# We then use the upwelling index values created above as a second filter to determine the metrics of the upwelling
# Unfortunately the way that the heatwaveR functions work well does with purrr. 
# I've figured this out once before but can't remember where now.
# Rather than go through the process I have rather just made a convenience function below that
# does what I want and can simply be given to purrr::map() without any faffing about.


# Detect event: 
detect_event_custom <- function(df){
  res <- detect_event(df, threshClim2 = df$exceedance, minDuration = 3, coldSpells = T)
  return(res)
}
# Calculate the upwelling event metrics
SACTN_upwell_base <- SACTN_US %>% 
  dplyr::rename(t = date) %>% 
  group_by(site) %>% 
  nest() %>% 
  # The climatology base period used here is up for debate...
  # The choice of the 30th percentile threshold also needs to be justified and sensitivty tested
  mutate(clim = purrr::map(data, ts2clm, pctile = 25, climatologyPeriod = c("1995-01-01", "2004-12-31"))) %>%
  select(-data) %>% 
  unnest() %>%
  left_join(SACTN_upwell, by = c("site", "t")) %>%
  filter(!is.na(exceedance)) %>%
  group_by(site) %>% 
  # mutate(thresh = mean(seas, na.rm = T)-sd(seas, na.rm = T)) %>% # Manually set threshold to the mean
  # mutate(thresh = 10) %>% # Manually set threshold to a static value. Doesn't work across all sites
  # mutate(thresh = quantile(temp, 0.3, na.rm = T)) %>% # Manually set threshold to a single quantile
  nest() %>% 
  mutate(exceed = purrr::map(data, detect_event_custom)) %>% 
  select(-data) #%>% 
  # unnest() %>% 
  # filter(row_number() %% 2 == 0) %>% # Select event summary metrics
  # filter(row_number() %% 2 == 1) %>% # Select daily values
  # unnest()
# Unpack the event metric reults
SACTN_upwell_events <- SACTN_upwell_base %>% 
  unnest() %>%
  filter(row_number() %% 2 == 0) %>%
  unnest()
# save(SACTN_upwell_events, file = "Data/SACTN_upwell_events.RData")
# Unpack the daily climatology results
SACTN_upwell_clims <- SACTN_upwell_base %>% 
  unnest() %>%
  filter(row_number() %% 2 == 1) %>% 
  unnest()
# save(SACTN_upwell_clims, file = "Data/SACTN_upwell_clims.RData")
# The above chunk of code appears to work as expected.
# What needs to be done now is that the thresholds for duration and UI strength need to be justified/decided uopn. 
    # Min duration for an upwelling event - 1day and threshold -1
# What also needs to be decided is if we are interested in any upwelling results throughout the year,
# or only during the upwelling season, whenever that may be for each site.
# I think we have demonstrated the technical capability needed to answer the question of whether or not upwelling
# is changing over time.
# Now we really need to focus on which of these parameters need to best be tweaked to answer that question.
```

Thereafter, I downloaded and then create a function which allows me to exctracted remotely sensed sea surface temperatures.Consequently we have a dataframe with a date, temperature, lat and lon columns as seen below.

```{r load_data2, include=TRUE}
load("../Data/site_list_sub.Rdata")
load(".../Data/SACTN_US.RData")
load("../Data/site_pixels.RData") # 5 decimal places
load("../Data/OISST.RData") # 2 decimal places
OISST <- BC_avhrr_only_v2_Document_Document 
rm(BC_avhrr_only_v2_Document_Document ); gc()
load("../Data/CMC.RData") # 1decimal places
```

The code below uses the extracted SST data and matches this to the lat and lon coordinates obtained at the earlier calculation in order to determine the temperature at a particular distance from the coastline. 

```{r matched_distance_coastline}
# # These following three objects (MUR, OISST, CMC) need to be the complete set of lon/lat values for your satellite data
# # MUR <- MUR %>% 
# #   select(lon, lat) %>% 
# #   mutate(product = "MUR")
# 
# # Decided to work with OISST and CMC as both has a time series of 30years
# OISST_prod <- OISST %>% 
#   select(lon, lat) %>% 
#   unique() %>% 
#   mutate(product = "OISST")
# 
# CMC_prod <- CMC %>% 
#   select(lon, lat) %>%
#   unique() %>% 
#   mutate(product = "CMC")
# 
# # sat_data <- rbind(CMC_prod, OISST_prod) %>% 
#   #rbind(., CMC) %>% 
#   # select(product, lon, lat)
# 
# # sat_pixels <- sat_data %>% 
#   # select(product, lon, lat) %>%
#   # unique()
# 
# ## For testing the nest/map pipeline
# # df <- site_pixels %>%
# #   filter(site == "Lamberts Bay") %>%
# #   select(-site)
# 
# match_func <- function(df){
#   df <- df %>%
#     dplyr::rename(lon_site = lon, lat_site = lat)
#   OISST_index <- OISST_prod[as.vector(knnx.index(as.matrix(OISST_prod[,c("lon", "lat")]),
#                                                  as.matrix(df[,c("lon_site", "lat_site")]), k = 1)),] %>% 
#     cbind(., df)
#   CMC_index <- CMC_prod[as.vector(knnx.index(as.matrix(CMC_prod[,c("lon", "lat")]),
#                                              as.matrix(df[,c("lon_site", "lat_site")]), k = 1)),] %>% 
#     cbind(., df)
#   res <- rbind(OISST_index, CMC_index)
#   return(res)
# }
# 
# # Find the nearest pixels to each spot along the transect
# # NB: SOme pixels are used more thanonce in a transect as the spacing isn't quite larger enough between transect
# # points to not fall inside of the same 25 KM pixel
# pixel_match <- site_pixels %>% 
#   group_by(site) %>% 
#   group_modify(~match_func(.x))
# 
# # You may then use the 'pixel_match' object to filter out the desired pixels from the full satellite products
# OISST_fill <- right_join(OISST, filter(pixel_match, product == "OISST"), by = c("lon", "lat"))
# CMC_fill <- right_join(CMC, filter(pixel_match, product == "CMC"), by = c("lon", "lat"))
# #SACTN_fill <- right_join(SACTN, filter(pixel_match, product == "SACTN"), by = c("lon", "lat"))
# 
# # Clean up some RAM space
# rm(OISST, CMC); gc()
# 
# # sites <- c("Port Nolloth", "Lamberts Bay", "Saldanha Bay", "Sea Point")
# # # In the SACTN dataset Hout Bay only has data until 2005. Hout Bay will now be removed from this study
# # # Hout Bay will be ignored
# selected_sites <- c("Port Nolloth", "Lamberts Bay", "Sea Point", "Saldanha Bay")
# 
# OISST_fill <- OISST_fill %>%
#   filter(site %in% selected_sites)
# 
# CMC_fill <- CMC_fill %>%
#   filter(site %in% selected_sites)
# 
# 
# # All the code below can be viewed in upwell_IDX.Rmd
# # Next to run upwelling index
# # UI Created using SAWS wind data (Find in upwell_IDX_Rmd)
# load("Data/UI_angle.RData")
# 
# upwelling <- UI_angle %>% 
#   dplyr::rename(temp = ui.saws) %>%
#   group_by(site) %>%
#   # mutate(min_t = min(t), 
#   #        max_t = max(t)) %>% 
#   nest() %>% # apply the following functions to all of the variables in te dataset
#   mutate(clim = purrr::map(data, ts2clm, climatologyPeriod = c("1997-01-01", "2015-12-31")), # creating a column of climatologies. Column will be named clim
#          # NB: A threshold of 3 appeared to be far to strict
#          # purr::map - apllies a function to each element of a vector
#          exceed = purrr::map(clim, exceedance, minDuration = 1, threshold = 1)) %>%  #Upwelling cannot be descrbed as an event. Upwelling can last for a few hours. Given that we have daily data, upwelling events minimum duration here will be 1day
#   # Detect consecutive days in exceedance of a given threshold.
#   # mutate() %>% 
#   select(-data, -clim) %>% 
#   unnest() %>%
#   filter(row_number() %% 2 == 1) %>%
#   unnest() %>% # creates a column for each variables
#   dplyr::rename(ui.saws = temp) %>% # rename upwelling index vale to temp so that it could work with the function
#   select(site, t, ui.saws, exceedance) 
# 
# # Now applying the Upwelling func
# # NB: This only pulls out the event results and not the climatology results
# # This is done to keep the output tidy because group_modify() may only create data.frame type outputs
# # To create a list output one would use group_map(), 
# # but this then loses the labels of which sites etc. the results belong to
# detect_event_custom <- function(df){
#   res <- detect_event(df, threshClim2 = df$exceedance, minDuration = 3, coldSpells = T)$event
#   return(res)
# }
# 
# ts2clm_custom <- function(df){
#   # The climatology base period used here is up for debate...
#   # The choice of the 25th percentile threshold also needs to be justified and sensitivty tested
#   res <- ts2clm(df, pctile = 25, climatologyPeriod = c("1992-01-01", "2016-12-31"))
#   return(res)
# }
# 
# # Calculate the upwelling event metrics
# upwelling_detect_event <- function(df){
#   upwell_base <- df %>% 
#     dplyr::rename(t = date) %>% 
#     group_by(site, product, heading, distance, lon, lat) %>% 
#     group_modify(~ts2clm_custom(.x)) %>% 
#     left_join(upwelling, by = c("site", "t")) %>%
#     filter(!is.na(exceedance)) %>%
#     group_by(site, product, heading, distance, lon, lat) %>% 
#     group_modify(~detect_event_custom(.x))
#   }
# 
# OISST_upwell_base <- upwelling_detect_event(df = OISST_fill)
# save(OISST_upwell_base, file = "Data/OISST_upwell_base.RData")
# CMC_upwell_base <- upwelling_detect_event(df = CMC_fill)
# save(CMC_upwell_base, file = "Data/CMC_upwell_base.RData")
# 
# # Here we remove the site Hout Bay so that we have a long time series. The length of Hout Bay time series ends in 200. Many sites change from here
# SACTN_US <- SACTN_US %>%
#   filter(site %in% selected_sites)
# 
# load("Data/site_list_v4.2.RData")
# 
# SACTN_upwell_base <- SACTN_US %>%
#   left_join(site_list[,c(4, 5, 6)], by = "index")%>% 
#     dplyr::rename(t = date) %>% 
#     group_by(site, lon, lat) %>% 
#     group_modify(~ts2clm_custom(.x)) %>% 
#     left_join(upwelling, by = c("site", "t")) %>%
#     filter(!is.na(exceedance)) %>%
#     group_by(site, lon, lat) %>% 
#     group_modify(~detect_event_custom(.x))
# 
# save(SACTN_upwell_base, file = "Data/SACTN_upwell_base.RData")
```


```{r loadfiles3, include=TRUE}
load("Data/OISST_upwell_base.RData")
load("Data/CMC_upwell_base.RData")
load("Data/SACTN_upwell_base.RData")
```

\subsubsection*{Analyses}



```{r}

lm_coeff <- function(df){
  res <- lm(formula = val ~ date_peak, data = df)
  res_coeff <- round(as.numeric(res$coefficient[2]), 4)
}
# Changes in upwelling metrics
lm_func <- function(df){
  upwell_lm <- df %>% 
  select(-c(index_start:index_end)) %>% 
  gather(key = "var", value = "val", -c(site:date_end)) %>% 
  group_by(site, var, distance) %>% 
  nest() %>% 
  mutate(slope = purrr::map(data, lm_coeff)) %>% 
  select(-data) %>% 
  unnest() %>% 
  # convert from daily to decadal values
  mutate(slope = slope * 365.25*10)
}

OISST_lm <- lm_func(df = OISST_upwell_base)
CMC_lm <- lm_func(df = CMC_upwell_base)

SACTN_lm <- SACTN_upwell_base %>% 
  select(-c(index_start:index_end)) %>% 
  gather(key = "var", value = "val", -c(site:date_end)) %>% 
  group_by(site, var) %>% 
  nest() %>% 
  mutate(slope = purrr::map(data, lm_coeff)) %>% 
  select(-data) %>% 
  unnest() %>% 
  # convert from daily to decadal values
  mutate(slope = slope * 365.25*10)
```

Here I do a three way ANOVA analayses. This allows me to compare one variable in two or more groups taking into account the variability of other variables. This analysis of covariance is used to analyse the relationship between the mean intensity  as a function of each site, year and duration.
 
```{r ANOVA analyses}
options(scipen = 999)
anova_func <- function(df){
  sites_aov <- aov(intensity_mean ~ site * duration * distance, data = df)
return(sites_aov)
}

OISST_anov <- anova_func(df = OISST_upwell_base)
summary(OISST_anov)
CMC_anov <- anova_func(df = CMC_upwell_base)
summary(CMC_anov)

SACTN_anov <- sites_aov <- aov(intensity_mean ~ site * duration, data = SACTN_upwell_base)
summary(SACTN_anov)
```

Since the analysis are largely dependant on wind data, I now continue with the analyses to show the various wind patterns that existed during this period. Here I created a function to plot the wind data by plotting wind rose diagrams.

```{r Waverose diagram, fig.keep = "none"}
library(ggradar)
library(dplyr)
library(scales)
library(tibble)
load("../Data/UI_angle.RData")

source("../Functions/wind.rose.R") 

wind_daily_renamed <- UI_angle %>% 
  mutate(dir_circ = ifelse(dir_circ < 0, dir_circ+360, dir_circ)) %>% 
  dplyr::rename(spd = mean_speed) %>%
  dplyr::rename(dir = dir_circ) %>% 
  filter(spd > 0)

p.wr2 <- plot.windrose(data = wind_daily_renamed,
              spd = "spd",
              dir = "dir")

p.wr3 <- p.wr2 + facet_wrap(.~ site, ncol = 2, nrow = 2) +
  theme(strip.text.x = element_text(size = 25)) + theme(panel.spacing = unit(2, "lines"))
p.wr3
```

The code below produces plots for each of the different datasets representing the cumulative intensity, peak date of event and duration of the detected event.

```{r loliplots, fig.keep="none"}
plot_loli_func <- function(df){
  ggplot(df, aes(x = date_peak, y = duration)) + 
  geom_lolli(aes(colour = intensity_cumulative)) + 
  scale_color_distiller(palette = "Spectral", name = "Cumulative \nintensity") + 
  facet_wrap(~distance, ncol = 2) +
  xlab("Date") + ylab("Event duration [days]") 
}

(OISST_SP_pl <- plot_loli_func(df = OISST_SEAPOINT))
(OISST_SB_pl <- plot_loli_func(df = OISST_SB))
(OISST_PN_pl <- plot_loli_func(df = OISST_PN))
(OISST_LB_pl <- plot_loli_func(df = OISST_LB))
OISST_SP_pl <- OISST_SP_pl +
  ggtitle("Sea Point")
OISST_SB_pl <- OISST_SB_pl +
  ggtitle("Saldanha Bay")
OISST_PN_pl <- OISST_PN_pl +
  ggtitle("Port Nolloth")
OISST_LB_pl <- OISST_LB_pl +
  ggtitle("Lamberts Bay")

combined_OISST_pl <- ggarrange(OISST_SP_pl, OISST_SB_pl, OISST_PN_pl,OISST_LB_pl)

#######
# CMC

(CMC_SP_pl <- plot_loli_func(df = CMC_SEAPOINT))
(CMC_SB_pl <- plot_loli_func(df = CMC_SB))
(CMC_PN_pl <- plot_loli_func(df = CMC_PN))
(CMC_LB_pl <- plot_loli_func(df = CMC_LB))

CMC_SP_pl <- CMC_SP_pl +
  ggtitle("Sea Point")
CMC_SB_pl <- CMC_SB_pl +
  ggtitle("Saldanha Bay")
CMC_PN_pl <- CMC_PN_pl +
  ggtitle("Port Nolloth")
CMC_LB_pl <- CMC_LB_pl +
  ggtitle("Lamberts Bay")

combined_CMC_pl <- ggarrange(CMC_SP_pl, CMC_SB_pl, CMC_PN_pl,CMC_LB_pl)

plot_loli_func <- function(df){
  ggplot(df, aes(x = date_peak, y = duration)) + 
  geom_lolli(aes(colour = intensity_cumulative)) + 
  scale_color_distiller(palette = "Spectral", name = "Cumulative \nintensity") + 
  facet_wrap(~site, ncol = 2) +
  xlab("Date") + ylab("Event duration [days]") 
}
(SACTN_plot <- plot_loli_func(df = SACTN_filtered))
```

\subsection*{References}
